# Comment out until feedback on https://github.com/SciPhi-AI/R2R/issues/1690

# [embedding]
# provider = "ollama"
# base_model = "ollama/mxbai-embed-large"
# base_dimension = 1_024
# batch_size = 32
# add_title_as_prefix = true

# [completion]
# provider = "litellm"
# concurrent_request_limit = 16 # defaults to 256

#     [completion.generation_config]
#     model = "ollama/llama3.2"
#     temperature = 0.1
#     top_p = 1
#     max_tokens_to_sample = 1_024
#     stream = true
#     add_generation_kwargs = {}
